{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5654d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import wordcloud\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "skl_stopwords=_stop_words.ENGLISH_STOP_WORDS\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47b75b",
   "metadata": {},
   "source": [
    "## Creating a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b98c8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(folders, base_url):\n",
    "    \"\"\"This functions creates corpus(list) of all the lyrics for given artists\n",
    "       This function required 2 input arguments:\n",
    "           1. A list of all the artists(folder_name) where the lyrics are stored\n",
    "           2. base_url where the lyrics folder is stored\n",
    "           \n",
    "               eg: folder = ['Eminem','Jay-Z','Justin-Timberlake','50-Cent','Bob-Marley','Michael-Jackson']\n",
    "                   base_url =r\"C:/Users/Asus/OneDrive/Desktop/Spiced_Academy/naive-zatar-student-codes/week4/\"\n",
    "                   \"\"\"\n",
    "    \n",
    "    #folders = ['Eminem','Jay-Z','Justin-Timberlake','50-Cent','Bob-Marley','Michael-Jackson']\n",
    "    global corpus \n",
    "    corpus = []# initilizing corpus empty list \n",
    "    global file_list_\n",
    "    file_list_ = [] #initilizing lyrics files empty list\n",
    "    global folder_len\n",
    "    folder_len = [] # initilizing empty folder_len list\n",
    "     \n",
    "    #base_url = r\"C:/Users/Asus/OneDrive/Desktop/Spiced_Academy/naive-zatar-student-codes/week4/\"\n",
    "\n",
    "    for folder in folders:\n",
    "        full_path = base_url+folder\n",
    "        #file_list_[folder] = os.listdir(full_path)\n",
    "        folder = os.listdir(full_path)\n",
    "        folder_len.append(len(folder))\n",
    "        #print(folder)\n",
    "        for filename in folder:\n",
    "            final_path = full_path+'/'+ filename\n",
    "            #print(final_path)\n",
    "\n",
    "            with open(final_path, 'r',encoding=\"utf8\") as f:\n",
    "                bob_marley = f.read()\n",
    "                corpus.append(bob_marley)\n",
    "                f.close()\n",
    "    global labels\n",
    "    labels = [] # initilizing empty labels list\n",
    "    for name , i in zip(folders, folder_len):\n",
    "        labels.append([name]*i)\n",
    "    labels = [ item for elem in labels for item in elem]  \n",
    "    \n",
    "    \n",
    "    return len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26fddde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['Eminem','Jay-Z','Justin-Timberlake','50-Cent','Bob-Marley','Michael-Jackson']\n",
    "base_url = r\"C:/Users/Asus/OneDrive/Desktop/Spiced_Academy/naive-zatar-student-codes/week4/\"\n",
    "create_corpus(folders, base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade0f33",
   "metadata": {},
   "source": [
    "## Processing the text by tokenizing and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9722aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_char(text_list):\n",
    "    global Corpus\n",
    "    Corpus = []\n",
    "    for i in text_list:\n",
    "        newstr = i.replace(\"\\n\", \" \")\n",
    "        Corpus.append(newstr)\n",
    "    return(len(Corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da34159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(text, stopwords=skl_stopwords, tokenizer=tokenizer, lemmatizer=lemmatizer):\n",
    "    panc = string.punctuation + '–'+ '‘'+ '’'+ '“'+'”'\n",
    "    text = [i for i in text if not re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\",i)]\n",
    "    text = ''.join([ch for ch in text if ch not in panc]) #remove punctuation\n",
    "    text = re.sub(pattern= '[0-9]+', string= text, repl = ' ' )\n",
    "    text = re.sub(pattern = '(aah|aaaa|aa)', string = text, repl ='')\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14609acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_char(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59e71cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize_lemmatize,encoding='utf-8', stop_words=None)\n",
    "X = vectorizer.fit_transform(Corpus)\n",
    "lyric_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d453c369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eminem', 'Jay-Z', 'Justin-Timberlake', '50-Cent', 'Bob-Marley', 'Michael-Jackson']\n"
     ]
    }
   ],
   "source": [
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d62770d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.DataFrame(labels, columns=['artist']) # inserting label col to the lyric_df dataframe.\n",
    "Labels['artist'] = Labels['artist'].map({'Eminem':0,'Jay-Z':1,'Justin-Timberlake':2,'50-Cent':3, 'Bob-Marley':4, 'Michael-Jackson':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36466c4",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c21c3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_over_sample(data_frame, labels):\n",
    "    \"\"\"This function return oversampled dataframe for imbalanced data\n",
    "        Required two input:\n",
    "        1. data_frame will be independent variable\n",
    "        2. labels will be dependent variable\"\"\"\n",
    "    \n",
    "    global X_ros\n",
    "    global y_ros\n",
    "    ros = RandomOverSampler(random_state=10, sampling_strategy=\"auto\")\n",
    "    X_ros, y_ros = ros.fit_resample(lyric_df, Labels)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "962560ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique samples in each class (oversampled): \n",
      "artist\n",
      "0         579\n",
      "1         579\n",
      "2         579\n",
      "3         579\n",
      "4         579\n",
      "5         579\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "random_over_sample(lyric_df, Labels)\n",
    "print(f'unique samples in each class (oversampled): \\n{y_ros.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80557078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2084, 22833), (1390, 22833), (2084, 1), (1390, 1))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_ros, y_ros, test_size=.40, random_state= 42)\n",
    "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a72980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((695, 22833), (695, 22833), (695, 1), (695, 1))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Xval, X_test, yval, y_test = train_test_split(Xtest, ytest, test_size=.50, random_state= 42)\n",
    "Xval.shape, X_test.shape, yval.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dd113aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfTransformer() \n",
    "Xtrain_norm = tf.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01dcb5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2084, 22833)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_norm=pd.DataFrame(Xtrain_norm.todense(), columns=vectorizer.get_feature_names_out())\n",
    "Xtrain_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac2efb",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39aa518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c203606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#n_estimators= 5000,min_samples_split= 350,min_samples_leaf= 7, max_depth=1000, random_state=101\n",
    "RFC  = RandomForestClassifier(n_estimators=1000,min_samples_split=250,min_samples_leaf=30, max_depth=1000, random_state=101).fit(Xtrain_norm, ytrain.values.ravel())\n",
    "\n",
    "# .values will give the values in a numpy array (shape: (n,1))\n",
    "# .ravel will convert that array shape to (n, ) (i.e. flatten it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "710d5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval_norm = tf.transform(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f4f8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval_norm=pd.DataFrame(Xval_norm.todense(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7bd2bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_rfc = RFC.predict(Xval_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd01b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score 74.42%\n",
      "Validation score 72.09%\n"
     ]
    }
   ],
   "source": [
    "print(f'Training score {round(RFC.score(Xtrain_norm, ytrain)*100,2)}%')\n",
    "print(f'Validation score {round(RFC.score(Xval_norm, yval)*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71d9582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = tf.transform(X_test)\n",
    "X_test_norm=pd.DataFrame(X_test_norm.todense(), columns=vectorizer.get_feature_names_out())\n",
    "ypred_rfc = RFC.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56606ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score 74.42%\n",
      "Validation score 72.95%\n"
     ]
    }
   ],
   "source": [
    "print(f'Training score {round(RFC.score(Xtrain_norm, ytrain)*100,2)}%')\n",
    "print(f'Validation score {round(RFC.score(X_test_norm, y_test)*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5eb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
